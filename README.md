# Proyecto 2 - DetecciÃ³n Temprana de CÃ¡ncer de Mama con Deep Learning y Mezcla de Expertos (MoE)

## ğŸ¯ Objetivo del Proyecto

Desarrollar un sistema de diagnÃ³stico asistido mediante inteligencia artificial que permita **clasificar imÃ¡genes de ecografÃ­a mamaria** (BreastMNIST) en benignas o malignas. Como innovaciÃ³n, se utilizarÃ¡ una arquitectura **Mixture of Experts (MoE)** para mejorar la precisiÃ³n del modelo y su capacidad de generalizaciÃ³n.

---

## ğŸ§  Dataset

**Fuente:** [MedMNIST v2 - BreastMNIST](https://medmnist.com/)

- Formato: ImÃ¡genes en escala de grises 64x64
- Etiquetas:
  - `0`: LesiÃ³n benigna
  - `1`: LesiÃ³n maligna
- Divisiones:
  - Entrenamiento: 546 imÃ¡genes
  - ValidaciÃ³n: 78 imÃ¡genes
  - Prueba: 156 imÃ¡genes
- DistribuciÃ³n:
  - Benigno: 147
  - Maligno: 399 (âš ï¸ Dataset desbalanceado)

---

## ğŸ” AnÃ¡lisis Exploratorio

- VisualizaciÃ³n de imÃ¡genes por clase
- ImÃ¡genes promedio por clase
- DetecciÃ³n de desbalance de clases

---

## ğŸ§ª DiseÃ±o Experimental

### Modelos a Comparar

1. **CNN estÃ¡ndar**
2. **MoE (Mixture of Experts)** con:
   - 2 expertos
   - 4 expertos
   - 8 expertos

### MÃ©todos complementarios

- Data Augmentation (rotaciÃ³n, zoom, flips)
- Class Weights o sobremuestreo para manejar desbalance
- RegularizaciÃ³n: Dropout, BatchNormalization
- Early Stopping
- ReduceLROnPlateau

---

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

- F1-Score
- AUC-ROC
- Matriz de confusiÃ³n
- PrecisiÃ³n y Recall por clase
- GrÃ¡ficas de loss/accuracy por Ã©poca

---

## âš™ï¸ Herramientas

- Lenguaje: `Python 3.x`
- Frameworks: `TensorFlow/Keras` o `PyTorch`
- Otras librerÃ­as: `NumPy`, `Pandas`, `Matplotlib`, `Seaborn`, `Scikit-learn`
- Entorno: `Google Colab` o `Jupyter Notebook`

---

## ğŸš€ Entregables

- `notebook.ipynb` con el desarrollo completo
- CÃ³digo para la arquitectura MoE
- CÃ³digo para la **CNN baseline** (`models/cnn_baseline.py`)
- ComparaciÃ³n de rendimiento entre CNN y MoE
- Visualizaciones y anÃ¡lisis
- Informe final (PDF + fuente)

---

## ğŸ“ Estructura esperada

/proyecto2-breastmnist/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ breastmnist.npz
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ experimento_MoE_vs_CNN.ipynb
â”œâ”€â”€ models/
â”‚ â””â”€â”€ moe_model.py
â”‚ â””â”€â”€ cnn_baseline.py
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ dataloader.py
â”œâ”€â”€ README.md
â””â”€â”€ informe_proyecto2.pdf

---

## ğŸ’¡ Bonus: InnovaciÃ³n

Se implementarÃ¡ una **Mezcla de Expertos (MoE)** para aprovechar mÃºltiples sub-modelos con gating network. Esta tÃ©cnica estÃ¡ bonificada con **+0.4 puntos extra** si se implementa correctamente.
